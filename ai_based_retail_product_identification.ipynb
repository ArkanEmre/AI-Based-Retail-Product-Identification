{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ai_based_retail_product_identification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1ButHBmXEBgklRPUxJEYT6hHOa3IpGAKc",
      "authorship_tag": "ABX9TyOphnc4kdQDcuXjK6T2f1Z3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArkanEmre/AI-Based-Retail-Product-Identification/blob/master/ai_based_retail_product_identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3_Sz_F2pEPG",
        "colab_type": "text"
      },
      "source": [
        "#Preprocessing\n",
        "\n",
        "##Moving rawdata to a single folder and creating an inVitro.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq36Lkg5cOEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from shutil import copyfile\n",
        "import csv\n",
        "\n",
        "dataset_path = '/content/drive/My Drive/Colab Notebooks/datasets/grozi120'\n",
        "new_path = '/content/drive/My Drive/Colab Notebooks/datasets/grozi120/processed'\n",
        "\n",
        "if not os.path.exists(os.path.join(new_path, 'inVitro')):\n",
        "    os.makedirs(os.path.join(new_path, 'inVitro'))\n",
        "\n",
        "with open(os.path.join(new_path, 'inVitro', 'inVitro.csv'), mode='w') as dataset_file:\n",
        "  dataset_writer = csv.writer(dataset_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "  dataset_writer.writerow(['file_name', 'class'])\n",
        "  for product in os.listdir(os.path.join(dataset_path, 'inVitro')):\n",
        "    for product_file in os.listdir(os.path.join(dataset_path, 'inVitro', product, 'web', 'JPEG')):\n",
        "      if product_file == 'Thumbs.db':\n",
        "        continue\n",
        "      dataset_writer.writerow(['{}_{}'.format(product, product_file[3:]), product])\n",
        "      copyfile(os.path.join(dataset_path, 'inVitro', product, 'web', 'JPEG', product_file), os.path.join(new_path, 'inVitro', '{}_{}'.format(product, product_file[3:])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_1GfGqTvrJS",
        "colab_type": "text"
      },
      "source": [
        "### Splitting data into train and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hzn4cQqpUaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "data_csv = pd.read_csv(os.path.join(new_path, 'inVitro', 'inVitro.csv'))\n",
        "\n",
        "# Prepend image filenames in train/ with relative path\n",
        "filenames = data_csv['file_name'].tolist()\n",
        "labels = data_csv['class'].tolist()\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "  (tf.constant(filenames), tf.constant(labels))\n",
        ")\n",
        "\n",
        "test_split, valid_split, train_split = tfds.Split.TRAIN.subsplit([10, 15, 75])\n",
        "\n",
        "dataset = dataset.shuffle(10)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}